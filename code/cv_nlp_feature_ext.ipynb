{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4810381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\py\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.vision_transformer import _cfg, PatchEmbed\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import itertools\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754279f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# attentionå±‚\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.attn_gradients = None\n",
    "        self.attention_map = None\n",
    "\n",
    "    def save_attn_gradients(self, attn_gradients):\n",
    "        self.attn_gradients = attn_gradients\n",
    "\n",
    "    def get_attn_gradients(self):\n",
    "        return self.attn_gradients\n",
    "\n",
    "    def save_attention_map(self, attention_map):\n",
    "        self.attention_map = attention_map\n",
    "\n",
    "    def get_attention_map(self):\n",
    "        return self.attention_map\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        B, N, C = x.shape\n",
    "        # (B, N, C) -> (B, N, 3, num_heads, nums_per_head) -> (3,B,num_heads,N,nums_per_head)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # (B,num_heads,N,nums_per_head)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "        # (B,num_heads,N,nums_per_head) @ (B,num_heads,nums_per_head,N) -> (B,num_heads,N,N)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        # softmax(Q @ K.T)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        if register_hook:\n",
    "            self.save_attention_map(attn)\n",
    "            attn.register_hook(self.save_attn_gradients)\n",
    "            # (B,num_heads,N,N) @ (B,num_heads,N,nums_per_head) -> (B,num_heads,N,nums_per_head) -> (B,N,num_heads,nums_per_head) -> (B,N,C)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        # linear projection\n",
    "        x = self.proj(x)\n",
    "        # linear dropout\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), register_hook=register_hook))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Vit\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`  -\n",
    "        https://arxiv.org/abs/2010.11929\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def forward(self, x, register_blk=-1):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, register_blk == i)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_pos_embed(pos_embed_checkpoint, visual_encoder):\n",
    "    # interpolate position embedding\n",
    "    embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "    #\n",
    "    num_patches = visual_encoder.patch_embed.num_patches\n",
    "    # pos_embed.shape == (1, num_patches + 1, embed_dim)\n",
    "    num_extra_tokens = visual_encoder.pos_embed.shape[-2] - num_patches  # 1\n",
    "    # height (== width) for eckpoint position embedding\n",
    "    #     # pos_embed_checkpoint.shape =the ch= (1,197,768)\n",
    "    # 196 ** 0.5 = 14 :orig_size\n",
    "    orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "    # height (== width) for the new position embedding\n",
    "    new_size = int(num_patches ** 0.5)\n",
    "    # patches\n",
    "    if orig_size != new_size:\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "        # only the position tokens are interpolated\n",
    "        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "        pos_tokens = torch.nn.functional.interpolate(\n",
    "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "        # (b,c,h,w)->(b,h,w,c)->(b,h*w,c)\n",
    "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "        #\n",
    "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "        print('reshape position embedding from %d to %d' % (orig_size ** 2, new_size ** 2))\n",
    "\n",
    "        return new_pos_embed\n",
    "    else:\n",
    "        return pos_embed_checkpoint\n",
    "\n",
    "\n",
    "def createVisualModel(resolution_after=224):\n",
    "    visual_encoder = VisionTransformer(\n",
    "        img_size=resolution_after, patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
    "        map_location=\"cpu\", check_hash=True)\n",
    "    state_dict = checkpoint[\"model\"]\n",
    "    pos_embed_reshaped = interpolate_pos_embed(state_dict['pos_embed'], visual_encoder)\n",
    "    state_dict['pos_embed'] = pos_embed_reshaped\n",
    "    visual_encoder.load_state_dict(state_dict, strict=False)\n",
    "    return visual_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece03c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_encoder = createVisualModel(resolution_after=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad328e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054b9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet's parameters\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018641e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(smiles, visual_encoder):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    molecule_image = Draw.MolToImage(molecule, size=(224, 224))\n",
    "    molecule_image_pil = molecule_image.convert('RGB')\n",
    "    molecule_image_tensor = transform(molecule_image_pil)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    molecule_image_tensor = molecule_image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # Get features from the visual_encoder\n",
    "    with torch.no_grad():  # Ensure no gradients are computed to save memory and computations\n",
    "        features = visual_encoder(molecule_image_tensor)\n",
    "\n",
    "    # Detach the tensor and convert to numpy\n",
    "    features_numpy = features.cpu().detach().numpy()\n",
    "    return features_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73843395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1843b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_feature(smiles):\n",
    "    inputs = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    cls_embedding = hidden_states[:, 0, :]\n",
    "    return(cls_embedding )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c85088",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'C/C(=C\\\\c1csc(C)n1)[C@@H]1C[C@@H]2O[C@]2(C)CCC[C@H](C)[C@H](O)[C@@H](C)C(=O)C(C)(C)[C@@H](O)CC(=O)N1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56c7a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 197, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_feature(a, visual_encoder).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e9a1356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_feature(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea471af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = np.zeros((merged_df.shape[0], 197, 768))\n",
    "\n",
    "for i, smile in enumerate(merged_df.SMILES):\n",
    "    image_features[i] = image_feature(smile, visual_encoder).squeeze(0)\n",
    "    \n",
    "import h5py\n",
    "\n",
    "with h5py.File('test_image_features.h5', 'w') as hf:\n",
    "    hf.create_dataset('test_image_features', data=image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_features = pd.DataFrame(0, range(merged_df.shape[0]), range(768))\n",
    "for i in range(merged_df.shape[0]):\n",
    "    A = string_feature(merged_df.SMILES[i])\n",
    "    A_np = A.squeeze(0).detach().numpy()\n",
    "    string_features.iloc[i] = A_np\n",
    "    \n",
    "string_features.to_csv('test_string_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
