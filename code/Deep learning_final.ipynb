{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f9cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\py\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, Dropout, BatchNormalization, SimpleRNN\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import itertools\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "da = pd.read_parquet('data/de_train.parquet')\n",
    "drug_smile = np.unique(da.SMILES)\n",
    "y = da.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe4fd5",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f9351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrrmse(y_true, y_pred):\n",
    "    # 计算每一行的均方误差\n",
    "    mse_per_row = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
    "    # 计算每一行的均方根误差\n",
    "    rmse_per_row = tf.sqrt(mse_per_row)\n",
    "    # 计算所有行的均值\n",
    "    mrrmse = tf.reduce_mean(rmse_per_row)\n",
    "    return mrrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfd8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_train = pd.read_csv('feature test/Half_fea_train_500.csv')\n",
    "attention_test = pd.read_csv('feature test/Half_fea_test_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a413ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(attention_train.columns == attention_test.columns) == attention_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_f = attention_train .shape[1]\n",
    "n_r = y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8280f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\py\\Anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_f = attention_train .shape[1]\n",
    "n_r = y.shape[1]\n",
    "\n",
    "def create_rnn_model(n_features = n_f, n_responses = n_r):\n",
    "    model = Sequential()\n",
    "    # Set return_sequences=True in the first RNN layer to output sequences for the next RNN layer\n",
    "    model.add(SimpleRNN(256, activation='tanh', return_sequences=True, input_shape=(1, n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    # The second RNN layer can have return_sequences=True since we want to maintain the sequence for batch normalization\n",
    "    model.add(SimpleRNN(128, activation='tanh', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    # The last RNN layer outputs the final sequence, so return_sequences is set to False\n",
    "    model.add(SimpleRNN(128, activation='tanh', return_sequences=False))\n",
    "    model.add(Dense(n_responses))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=mrrmse)\n",
    "    return model\n",
    "\n",
    "# 定义交叉验证参数\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# 用于存储每次迭代的分数\n",
    "scores = []\n",
    "attention_train_reshaped = np.expand_dims(attention_train, axis=1)\n",
    "\n",
    "for train, test in kfold.split(attention_train_reshaped, y):\n",
    "    model = create_rnn_model()\n",
    "    model.fit(attention_train_reshaped[train], y.iloc[train], epochs=10, batch_size=32, verbose=0)\n",
    "    scores.append(model.evaluate(attention_train_reshaped[test], y.iloc[test], verbose=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccb6f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 7ms/step\n",
      "(255, 18211)\n"
     ]
    }
   ],
   "source": [
    "attention_test_reshaped = np.expand_dims(attention_test, axis=1)\n",
    "predictions = model.predict(attention_test_reshaped)\n",
    "print(predictions.shape)\n",
    "# submission.iloc[:, 1:] = predictions\n",
    "\n",
    "# submission.to_csv('RNN_chem_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a125864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv') \n",
    "submission.iloc[:, 1:] = predictions\n",
    "submission.to_csv('RNN_final_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d86dc",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d6f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 10ms/step\n",
      "(255, 18211)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "def create_complex_gru_model(n_features = n_f, n_responses = n_r):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(512, activation='tanh', return_sequences=True, input_shape=(1, n_features),\n",
    "                  kernel_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(GRU(256, activation='tanh', return_sequences=True,\n",
    "                  kernel_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GRU(256, activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(GRU(128, activation='tanh', return_sequences=False))\n",
    "    # Adding a TimeDistributed Dense layer to operate on each time step; only makes sense if return_sequences=True in the last GRU layer\n",
    "    # model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "    model.add(Dense(128, activation='tanh'))\n",
    "    model.add(Dense(n_responses))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=mrrmse)\n",
    "    return model\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "scores = []\n",
    "# cv\n",
    "for train, test in kfold.split(attention_train, y):\n",
    "    model = create_complex_gru_model()\n",
    "    model.fit(attention_train_reshaped[train], y.iloc[train], epochs=10, batch_size=32, verbose=0)\n",
    "    scores.append(model.evaluate(attention_train_reshaped[test], y.iloc[test], verbose=0))\n",
    "    \n",
    "    \n",
    "attention_test_reshaped = np.expand_dims(attention_test, axis=1)\n",
    "predictions = model.predict(attention_test_reshaped)\n",
    "print(predictions.shape)\n",
    "\n",
    "submission.iloc[:, 1:] = predictions\n",
    "submission.to_csv('GRU_final_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011a465",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04c675a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 5ms/step\n",
      "(255, 18211)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def create_lstm_model(n_features, n_responses):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with return_sequences=True to output sequences for the next LSTM layer\n",
    "    model.add(LSTM(256, activation='tanh', return_sequences=True, input_shape=(1, n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(128, activation='tanh', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    # Last LSTM layer, outputs final sequence, so return_sequences=False\n",
    "    model.add(LSTM(128, activation='tanh', return_sequences=False))\n",
    "    model.add(Dense(n_responses))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')  # Replace 'mrrmse' with your loss function\n",
    "    return model\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "scores = []\n",
    "# cv\n",
    "for train, test in kfold.split(attention_train, y):\n",
    "    model = create_lstm_model(n_f, n_r)\n",
    "    model.fit(attention_train_reshaped[train], y.iloc[train], epochs=10, batch_size=32, verbose=0)\n",
    "    scores.append(model.evaluate(attention_train_reshaped[test], y.iloc[test], verbose=0))\n",
    "    \n",
    "    \n",
    "attention_test_reshaped = np.expand_dims(attention_test, axis=1)\n",
    "predictions = model.predict(attention_test_reshaped)\n",
    "print(predictions.shape)\n",
    "\n",
    "submission.iloc[:, 1:] = predictions\n",
    "submission.to_csv('LSTM_final_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fde85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
